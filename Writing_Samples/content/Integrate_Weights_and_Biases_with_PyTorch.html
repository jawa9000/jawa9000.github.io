<!DOCTYPE html>
<html>

<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-06DGRLH7V4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-06DGRLH7V4');
    </script>
    <title>Integrate Weights and Biases with PyTorch</title>
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">
    <link href="../../css/bootstrap.css" rel="stylesheet">
    <link href="../../css/tutorials.css" rel="stylesheet">
    <link rel="stylesheet" href="../styles/main.css" type="text/css" />
    <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script src="../../js/jquery-3.7.1.min.js"></script>
</head>

<body>
    <div class="container">
        <div class="row">
            <div class="col-md-1"></div>
            <div class="col-md-10">
                <h1 id="title-heading">Integrate Weights & Biases with PyTorch</h1>
                 <p class="note">Archive note: all links and images have been disabled as their destinations and/or source locations no longer exist. The main navigation is also missing.</p>
                <div>Rewritten and edited by Brian Immel, last modified on Dec 28, 2025</div>
                <toc></toc>
                <p>This guide demonstrates how to integrate <a href="#">Weights & Biases (W&B)</a> into your PyTorch pipeline. W&B helps you track machine learning experiments, visualize model performance, and ensure reproducibility across teams.</p>

                <h2>Before you begin</h2>
                <p>This guide is intended for machine learning engineers and data scientists who are familiar with PyTorch and Python. Using W&B allows you to move beyond manual logging in spreadsheets to an automated, central dashboard for all your model metadata.</p>

                <p>By the end of this guide, you will know how to:</p>
                <ul>
                    <li>Initialize a W&B run and configure hyperparameters.</li>
                    <li>Define and "watch" a PyTorch model for gradient tracking.</li>
                    <li>Track real-time metrics and save model artifacts for reproducibility.</li>
                </ul>

                <h2>Prerequisites</h2>
                <ul>
                    <li>A W&B account.</li>
                    <li>A Python environment with <code>torch</code> and <code>torchvision</code> installed.</li>
                </ul>

                <h2>Step 1: Install and Authenticate</h2> 
                <p>Install the <code>wandb</code> and <code>onnx</code> libraries and log in to your account. </p>
                <pre><code>
# Install dependencies
pip install wandb onnx -Uq

import wandb
wandb.login()</code></pre> 

                <h2>Step 2: Define Model and Data</h2>
                <p>The following boilerplate defines a standard Convolutional Neural Network (CNN) and data loaders for the MNIST dataset.</p>
                <pre><code>
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms

# Select device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ConvNet(nn.Module):
    def __init__(self, kernels, classes):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(kernels[0], kernels[1], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7 * 7 * kernels[1], classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out

def make_loader(config, train=True):
    full_dataset = torchvision.datasets.MNIST(root=".", train=train, transform=transforms.ToTensor(), download=True)
    return torch.utils.data.DataLoader(dataset=full_dataset, batch_size=config.batch_size, shuffle=True, pin_memory=True, num_workers=2)</code></pre>

                <h2>Step 3: Initialize the Run and Hyperparameters</h2>
                <p>Initialize a W&B run to track your experiment. Use the <code>config</code> dictionary to capture hyperparameters, which allows you to filter and query runs in the W&B dashboard.</p>
                <pre><code>
# Define experiment metadata and hyperparameters
config = {
    "epochs": 5,
    "classes": 10,
    "kernels": [16, 32],
    "batch_size": 128,
    "learning_rate": 0.005,
    "dataset": "MNIST",
    "architecture": "CNN"
}

def model_pipeline(hyperparameters):
    # Initialize a new W&B run.
    with wandb.init(project="&lt;PROJECT_NAME&gt;", config=hyperparameters) as run:
        config = run.config

        # Build model, data, and optimizer
        train_loader = make_loader(config, train=True)
        test_loader = make_loader(config, train=False)
        model = ConvNet(config.kernels, config.classes).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)

        # Train and track performance
        train(model, train_loader, criterion, optimizer, config)

        # Evaluate final performance
        test(model, test_loader)

    return model</code></pre>

                <h2>Step 4: Track Metrics and Gradients</h2>
                <p>Use <code>wandb.watch</code> to log model gradients and <code>wandb.log</code> to capture training metrics such as loss and accuracy.</p>

                <pre><code>
def train(model, loader, criterion, optimizer, config):
    # Log gradients and topology
    wandb.watch(model, criterion, log="all", log_freq=10)

    example_ct = 0
    for epoch in range(config.epochs):
        for images, labels in loader:
            loss = train_batch(images, labels, model, optimizer, criterion)
            example_ct += len(images)
            # Log metrics to the dashboard every 25 batches
            wandb.log({"epoch": epoch, "loss": loss}, step=example_ct)

def train_batch(images, labels, model, optimizer, criterion):
    images, labels = images.to(device), labels.to(device)
    outputs = model(images)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss</code></pre>

                <h2>Step 5: Version and Save the Model</h2>
                <p>Export your model to the ONNX format and use <code>wandb.save</code> to upload the file. This associates the model artifact directly with the training run.</p>
                <pre><code>
def test(model, test_loader):
    model.eval()
    with torch.no_grad():
        correct, total = 0, 0
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        accuracy = correct / total
        wandb.log({"test_accuracy": accuracy})

    # Save the model
    torch.onnx.export(model, images, "model.onnx")
    wandb.save("model.onnx")
                </code></pre>
                <h2>Best Practices for Reproducibility</h2>
                <p>To ensure deterministic behavior across your experiments, it is essential to set random seeds for all libraries involved in the computation.</p>
                
                <p class="note"><strong>Note:</strong> While setting seeds improves reproducibility, some GPU operations remain non-deterministic. Results may still vary slightly across different hardware configurations. See the <a href="https://pytorch.org/docs/stable/notes/randomness.html" target="_blank">PyTorch randomness guide</a> for details.</p>

                <pre><code>
import random
import numpy as np
import torch

# Set random seeds
random.seed(hash("setting random seeds") % 2**32 - 1)
np.random.seed(hash("improves reproducibility") % 2**32 - 1)
torch.manual_seed(hash("by removing variation") % 2**32 - 1)
torch.cuda.manual_seed_all(hash("across runs") % 2**32 - 1)
                </code></pre>
                <h2>Advanced Configuration</h2>
                <h3>Hyperparameter Sweeps</h3>
                <p>Automate model tuning by defining a search strategy and running an agent.</p>
                <pre><code>
# Initialize and run a sweep
sweep_id = wandb.sweep(sweep_config)
wandb.agent(sweep_id, function=train)
</code></pre>
                <h3>Environment Settings</h3>
                <ul>
                    <li><strong>Offline Mode</strong>: Set <code>WANDB_MODE=dryrun</code> to train without an internet connection. Use <code>wandb sync</code> to upload your data later.</li>
                    <li><strong>Headless Authentication</strong>: Use the <code>WANDB_API_KEY</code> environment variable to authenticate on automated managed clusters.</li>
                </ul>

                <h2>Next steps</h2>
                <ul>
                    <li>Learn how to use <a href="#">W&B Artifacts</a> for dataset versioning.</li>
                    <li>Explore more <a href="#">W&B PyTorch Examples</a> on GitHub.</li>
                </ul>
            </div>
            <div class="col-md-1"></div>
        </div>
    </div>
</body>
<script src="../../js/toc_generator.js"></script>
</html>